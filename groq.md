---
description: Get up and running with the Groq API in minutes: create an API key, set up your environment, and make your first request.
title: Quickstart - GroqDocs
image: https://console.groq.com/og_cloudv5.jpg
---

# Quickstart

Get up and running with the Groq API in a few minutes, with the steps below.

For additional support, catch our [onboarding video](https://console.groq.com/docs/overview).

## [Create an API Key](#create-an-api-key)

Please visit [here](https://console.groq.com/keys) to create an API Key.

## [Set up your API Key (recommended)](#set-up-your-api-key-recommended)

Configure your API key as an environment variable. This approach streamlines your API usage by eliminating the need to include your API key in each request. Moreover, it enhances security by minimizing the risk of inadvertently including your API key in your codebase.

### [In your terminal of choice:](#in-your-terminal-of-choice)

shell

```
export GROQ_API_KEY=<your-api-key-here>
```

## [Requesting your first chat completion](#requesting-your-first-chat-completion)

curlJavaScriptPythonJSON

### [Install the Groq Python library:](#install-the-groq-python-library)

shell

```
pip install groq
```

### [Performing a Chat Completion:](#performing-a-chat-completion)

Python

```
import os

from groq import Groq

client = Groq(
    api_key=os.environ.get("GROQ_API_KEY"),
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Explain the importance of fast language models",
        }
    ],
    model="llama-3.3-70b-versatile",
)

print(chat_completion.choices[0].message.content)
```

## [Using third-party libraries and SDKs](#using-thirdparty-libraries-and-sdks)

Vercel AI SDKLiteLLMLangChain

### [Using AI SDK:](#using-ai-sdk)

[AI SDK](https://ai-sdk.dev/) is a Javascript-based open-source library that simplifies building large language model (LLM) applications. Documentation for how to use Groq on the AI SDK [can be found here](https://console.groq.com/docs/ai-sdk/).

  
First, install the `ai` package and the Groq provider `@ai-sdk/groq`:

  
shell

```
pnpm add ai @ai-sdk/groq
```

  
Then, you can use the Groq provider to generate text. By default, the provider will look for `GROQ_API_KEY` as the API key.

  
JavaScript

```
import { groq } from '@ai-sdk/groq';
import { generateText } from 'ai';

const { text } = await generateText({
  model: groq('llama-3.3-70b-versatile'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});
```

Now that you have successfully received a chat completion, you can try out the other endpoints in the API.

### [Next Steps](#next-steps)

* Check out the [Playground](https://console.groq.com/playground) to try out the Groq API in your browser
* Join our GroqCloud [developer community](https://community.groq.com/)
* Add a how-to on your project to the [Groq API Cookbook](https://github.com/groq/groq-api-cookbook)
* llama-3.3-70b-versatile
* qwen/qwen3-32b
Rate Limits
Copy page

Rate limits act as control measures to regulate how frequently users and applications can access our API within specified timeframes. These limits help ensure service stability, fair access, and protection against misuse so that we can serve reliable and fast inference for all.

Understanding Rate Limits
Rate limits are measured in:

RPM: Requests per minute
RPD: Requests per day
TPM: Tokens per minute
TPD: Tokens per day
ASH: Audio seconds per hour
ASD: Audio seconds per day
Cached tokens do not count towards your rate limits.

Rate limits apply at the organization level, not individual users. You can hit any limit type depending on which threshold you reach first.

Example: Let's say your RPM = 50 and your TPM = 200K. If you were to send 50 requests with only 100 tokens within a minute, you would reach your limit even though you did not send 200K tokens within those 50 requests.

Rate Limits
The following is a high level summary and there may be exceptions to these limits. You can view the current, exact rate limits for your organization on the limits page in your account settings.

Need higher rate limits? The limits shown below are the base limits for the Developer tier. Higher rate limits are available for select workloads and enterprise use cases. If your application requires increased capacity, you can request custom rate limit increases here.

Free Tier Limits
Developer Tier Limits
MODEL ID	RPM	RPD	TPM	TPD	ASH	ASD
allam-2-7b
300
60K
60K
-
-
-
groq/compound
200
20K
200K
-
-
-
groq/compound-mini
200
20K
200K
-
-
-
llama-3.1-8b-instant
1K
500K
250K
-
-
-
llama-3.3-70b-versatile
1K
500K
300K
-
-
-
meta-llama/llama-4-maverick-17b-128e-instruct
1K
500K
300K
-
-
-
meta-llama/llama-4-scout-17b-16e-instruct
1K
500K
300K
-
-
-
meta-llama/llama-guard-4-12b
100
50K
30K
1M
-
-
meta-llama/llama-prompt-guard-2-22m
100
50K
30K
-
-
-
meta-llama/llama-prompt-guard-2-86m
100
50K
30K
-
-
-
moonshotai/kimi-k2-instruct
1K
500K
250K
-
-
-
moonshotai/kimi-k2-instruct-0905
1K
500K
250K
-
-
-
openai/gpt-oss-120b
1K
500K
250K
-
-
-
openai/gpt-oss-20b
1K
500K
250K
-
-
-
openai/gpt-oss-safeguard-20b
1K
500K
150K
-
-
-
playai-tts
250
100K
50K
2M
-
-
playai-tts-arabic
250
100K
50K
2M
-
-
qwen/qwen3-32b
1K
500K
300K
-
-
-
whisper-large-v3
300
200K
-
-
200K
4M
whisper-large-v3-turbo
400
200K
-
-
400K
4M
Rate Limit Headers
In addition to viewing your limits on your account's limits page, you can also view rate limit information such as remaining requests and tokens in HTTP response headers as follows:

The following headers are set (values are illustrative):

Header	Value	Notes
retry-after	2	In seconds
x-ratelimit-limit-requests	14400	Always refers to Requests Per Day (RPD)
x-ratelimit-limit-tokens	18000	Always refers to Tokens Per Minute (TPM)
x-ratelimit-remaining-requests	14370	Always refers to Requests Per Day (RPD)
x-ratelimit-remaining-tokens	17997	Always refers to Tokens Per Minute (TPM)
x-ratelimit-reset-requests	2m59.56s	Always refers to Requests Per Day (RPD)
x-ratelimit-reset-tokens	7.66s	Always refers to Tokens Per Minute (TPM)
Handling Rate Limits
When you exceed rate limits, our API returns a 429 Too Many Requests HTTP status code.

Note: retry-after is only set if you hit the rate limit and status code 429 is returned. The other headers are always included.

Was this page helpful?

---
description: Comprehensive reference documentation for the Groq API, including endpoints, parameters, and examples.
title: API Reference - GroqDocs
---

# Groq API Reference

[Chat](https://console.groq.com/docs/api-reference#chat)

[Create chat completion](https://console.groq.com/docs/api-reference#chat-create)

POSThttps://api.groq.com/openai/v1/chat/completions

Creates a model response for the given chat conversation.

### 

[Request Body](https://console.groq.com/docs/api-reference#chat-create-request-body)

* messagesarrayRequired  
A list of messages comprising the conversation so far.  
### Show possible types
* modelstringRequired  
ID of the model to use. For details on which models are compatible with the Chat API, see available [models](https://console.groq.com/docs/models)
* citation\_optionsstring or nullOptionalDefaults to enabled  
Allowed values: `enabled, disabled`  
Whether to enable citations in the response. When enabled, the model will include citations for information retrieved from provided documents or web searches.
* compound\_customobject or nullOptional  
Custom configuration of models and tools for Compound.  
### Show properties
* disable\_tool\_validationbooleanOptionalDefaults to false  
If set to true, groq will return called tools without validating that the tool is present in request.tools. tool\_choice=required/none will still be enforced, but the request cannot require a specific tool be used.
* documentsarray or nullOptional  
A list of documents to provide context for the conversation. Each document contains text that can be referenced by the model.  
### Show properties
* exclude\_domainsDeprecatedarray or nullOptional  
Deprecated: Use search\_settings.exclude\_domains instead. A list of domains to exclude from the search results when the model uses a web search tool.
* frequency\_penaltynumber or nullOptionalDefaults to 0  
Range: \-2 - 2  
This is not yet supported by any of our models. Number between -2.0 and 2.0\. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
* function\_callDeprecatedstring / object or nullOptional  
Deprecated in favor of `tool_choice`.  
Controls which (if any) function is called by the model.`none` means the model will not call a function and instead generates a message.`auto` means the model can pick between generating a message or calling a function. Specifying a particular function via `{"name": "my_function"}` forces the model to call that function.  
`none` is the default when no functions are present. `auto` is the default if functions are present.  
### Show possible types
* functionsDeprecatedarray or nullOptional  
Deprecated in favor of `tools`.  
A list of functions the model may generate JSON inputs for.  
### Show properties
* include\_domainsDeprecatedarray or nullOptional  
Deprecated: Use search\_settings.include\_domains instead. A list of domains to include in the search results when the model uses a web search tool.
* include\_reasoningboolean or nullOptional  
Whether to include reasoning in the response. If true, the response will include a `reasoning` field. If false, the model's reasoning will not be included in the response. This field is mutually exclusive with `reasoning_format`.
* logit\_biasobject or nullOptional  
This is not yet supported by any of our models. Modify the likelihood of specified tokens appearing in the completion.
* logprobsboolean or nullOptionalDefaults to false  
This is not yet supported by any of our models. Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`.
* max\_completion\_tokensinteger or nullOptional  
The maximum number of tokens that can be generated in the chat completion. The total length of input tokens and generated tokens is limited by the model's context length.
* max\_tokensDeprecatedinteger or nullOptional  
Deprecated in favor of `max_completion_tokens`. The maximum number of tokens that can be generated in the chat completion. The total length of input tokens and generated tokens is limited by the model's context length.
* metadataobject or nullOptional  
This parameter is not currently supported.
* ninteger or nullOptionalDefaults to 1  
Range: 1 - 1  
How many chat completion choices to generate for each input message. Note that the current moment, only n=1 is supported. Other values will result in a 400 response.
* parallel\_tool\_callsboolean or nullOptionalDefaults to true  
Whether to enable parallel function calling during tool use.
* presence\_penaltynumber or nullOptionalDefaults to 0  
Range: \-2 - 2  
This is not yet supported by any of our models. Number between -2.0 and 2.0\. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
* reasoning\_effortstring or nullOptional  
Allowed values: `none, default, low, medium, high`  
qwen3 models support the following values Set to 'none' to disable reasoning. Set to 'default' or null to let Qwen reason.  
openai/gpt-oss-20b and openai/gpt-oss-120b support 'low', 'medium', or 'high'. 'medium' is the default value.
* reasoning\_formatstring or nullOptional  
Allowed values: `hidden, raw, parsed`  
Specifies how to output reasoning tokens This field is mutually exclusive with `include_reasoning`.
* response\_formatobject / object / object or nullOptional  
An object specifying the format that the model must output. Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema. `json_schema` response format is only available on [supported models](https://console.groq.com/docs/structured-outputs#supported-models). Setting to `{ "type": "json_object" }` enables the older JSON mode, which ensures the message the model generates is valid JSON. Using `json_schema` is preferred for models that support it.  
### Show possible types
* search\_settingsobject or nullOptional  
Settings for web search functionality when the model uses a web search tool.  
### Show properties
* seedinteger or nullOptional  
If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.
* service\_tierstring or nullOptional  
Allowed values: `auto, on_demand, flex, performance, null`  
The service tier to use for the request. Defaults to `on_demand`.  
   * `auto` will automatically select the highest tier available within the rate limits of your organization.  
   * `flex` uses the flex tier, which will succeed or fail quickly.
* stopstring / array or nullOptional  
Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.  
### Show possible types
* storeboolean or nullOptional  
This parameter is not currently supported.
* streamboolean or nullOptionalDefaults to false  
If set, partial message deltas will be sent. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent%5Fevents/Using%5Fserver-sent%5Fevents#Event%5Fstream%5Fformat) as they become available, with the stream terminated by a `data: [DONE]` message. [Example code](https://console.groq.com/docs/text-chat#streaming-a-chat-completion).
* stream\_optionsobject or nullOptional  
Options for streaming response. Only set this when you set `stream: true`.  
### Show properties
* temperaturenumber or nullOptionalDefaults to 1  
Range: 0 - 2  
What sampling temperature to use, between 0 and 2\. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top\_p but not both.
* tool\_choicestring / object or nullOptional  
Controls which (if any) tool is called by the model.`none` means the model will not call any tool and instead generates a message.`auto` means the model can pick between generating a message or calling one or more tools.`required` means the model must call one or more tools. Specifying a particular tool via `{"type": "function", "function": {"name": "my_function"}}` forces the model to call that tool.  
`none` is the default when no tools are present. `auto` is the default if tools are present.  
### Show possible types
* toolsarray or nullOptional  
A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.  
### Show properties
* top\_logprobsinteger or nullOptional  
Range: 0 - 20  
This is not yet supported by any of our models. An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.
* top\_pnumber or nullOptionalDefaults to 1  
Range: 0 - 1  
An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top\_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both.
* userstring or nullOptional  
A unique identifier representing your end-user, which can help us monitor and detect abuse.

### 

[Response Object](https://console.groq.com/docs/api-reference#chat-create-returns)

* choicesarray  
A list of chat completion choices. Can be more than one if `n` is greater than 1.  
### Show properties
* createdinteger  
The Unix timestamp (in seconds) of when the chat completion was created.
* idstring  
A unique identifier for the chat completion.
* mcp\_list\_toolsarray or null  
List of discovered MCP tools from connected servers.  
### Show properties
* modelstring  
The model used for the chat completion.
* objectstring  
Allowed values: `chat.completion`  
The object type, which is always `chat.completion`.
* service\_tierstring or null  
Allowed values: `auto, on_demand, flex, performance, null`  
The service tier used for the request.
* system\_fingerprintstring  
This fingerprint represents the backend configuration that the model runs with.  
Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.
* usageobject  
Usage statistics for the completion request.  
### Show properties
* usage\_breakdownobject  
Usage statistics for compound AI completion requests.  
### Show properties
* x\_groqobject  
Groq-specific metadata for non-streaming chat completion responses.  
### Show properties

curl

```
curl https://api.groq.com/openai/v1/chat/completions -s \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $GROQ_API_KEY" \
-d '{
  "model": "llama-3.3-70b-versatile",
  "messages": [{
      "role": "user",
      "content": "Explain the importance of fast language models"
  }]
}'
```

```
import Groq from "groq-sdk";

const groq = new Groq({ apiKey: process.env.GROQ_API_KEY });

async function main() {
  const completion = await groq.chat.completions
    .create({
      messages: [
        {
          role: "user",
          content: "Explain the importance of fast language models",
        },
      ],
      model: "llama-3.3-70b-versatile",
    })
  console.log(completion.choices[0].message.content);
}

main();
```

```
import os

from groq import Groq

client = Groq(
    # This is the default and can be omitted
    api_key=os.environ.get("GROQ_API_KEY"),
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "system",
            "content": "You are a helpful assistant."
        },
        {
            "role": "user",
            "content": "Explain the importance of fast language models",
        }
    ],
    model="llama-3.3-70b-versatile",
)

print(chat_completion.choices[0].message.content)
```

Example Response

```
{
  "id": "chatcmpl-f51b2cd2-bef7-417e-964e-a08f0b513c22",
  "object": "chat.completion",
  "created": 1730241104,
  "model": "openai/gpt-oss-20b",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Fast language models have gained significant attention in recent years due to their ability to process and generate human-like text quickly and efficiently. The importance of fast language models can be understood from their potential applications and benefits:\n\n1. **Real-time Chatbots and Conversational Interfaces**: Fast language models enable the development of chatbots and conversational interfaces that can respond promptly to user queries, making them more engaging and useful.\n2. **Sentiment Analysis and Opinion Mining**: Fast language models can quickly analyze text data to identify sentiments, opinions, and emotions, allowing for improved customer service, market research, and opinion mining.\n3. **Language Translation and Localization**: Fast language models can quickly translate text between languages, facilitating global communication and enabling businesses to reach a broader audience.\n4. **Text Summarization and Generation**: Fast language models can summarize long documents or even generate new text on a given topic, improving information retrieval and processing efficiency.\n5. **Named Entity Recognition and Information Extraction**: Fast language models can rapidly recognize and extract specific entities, such as names, locations, and organizations, from unstructured text data.\n6. **Recommendation Systems**: Fast language models can analyze large amounts of text data to personalize product recommendations, improve customer experience, and increase sales.\n7. **Content Generation for Social Media**: Fast language models can quickly generate engaging content for social media platforms, helping businesses maintain a consistent online presence and increasing their online visibility.\n8. **Sentiment Analysis for Stock Market Analysis**: Fast language models can quickly analyze social media posts, news articles, and other text data to identify sentiment trends, enabling financial analysts to make more informed investment decisions.\n9. **Language Learning and Education**: Fast language models can provide instant feedback and adaptive language learning, making language education more effective and engaging.\n10. **Domain-Specific Knowledge Extraction**: Fast language models can quickly extract relevant information from vast amounts of text data, enabling domain experts to focus on high-level decision-making rather than manual information gathering.\n\nThe benefits of fast language models include:\n\n* **Increased Efficiency**: Fast language models can process large amounts of text data quickly, reducing the time and effort required for tasks such as sentiment analysis, entity recognition, and text summarization.\n* **Improved Accuracy**: Fast language models can analyze and learn from large datasets, leading to more accurate results and more informed decision-making.\n* **Enhanced User Experience**: Fast language models can enable real-time interactions, personalized recommendations, and timely responses, improving the overall user experience.\n* **Cost Savings**: Fast language models can automate many tasks, reducing the need for manual labor and minimizing costs associated with data processing and analysis.\n\nIn summary, fast language models have the potential to transform various industries and applications by providing fast, accurate, and efficient language processing capabilities."
      },
      "logprobs": null,
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "queue_time": 0.037493756,
    "prompt_tokens": 18,
    "prompt_time": 0.000680594,
    "completion_tokens": 556,
    "completion_time": 0.463333333,
    "total_tokens": 574,
    "total_time": 0.464013927
  },
  "system_fingerprint": "fp_179b0f92c9",
  "x_groq": { "id": "req_01jbd6g2qdfw2adyrt2az8hz4w" }
}
```

[Responses (beta)](https://console.groq.com/docs/api-reference#responses)

[Create response](https://console.groq.com/docs/api-reference#responses-create)

POSThttps://api.groq.com/openai/v1/responses

Creates a model response for the given input.

### 

[Request Body](https://console.groq.com/docs/api-reference#responses-create-request-body)

* inputstring / arrayRequired  
Text input to the model, used to generate a response.  
### Show possible types
* modelstringRequired  
ID of the model to use. For details on which models are compatible with the Responses API, see available [models](https://console.groq.com/docs/models)
* instructionsstring or nullOptional  
Inserts a system (or developer) message as the first item in the model's context.
* max\_output\_tokensinteger or nullOptional  
An upper bound for the number of tokens that can be generated for a response, including visible output tokens and reasoning tokens.
* metadataobject or nullOptional  
Custom key-value pairs for storing additional information. Maximum of 16 pairs.
* parallel\_tool\_callsboolean or nullOptionalDefaults to true  
Enable parallel execution of multiple tool calls.
* reasoningobject or nullOptional  
Configuration for reasoning capabilities when using compatible models.  
### Show properties
* service\_tierstring or nullOptionalDefaults to auto  
Allowed values: `auto, default, flex`  
Specifies the latency tier to use for processing the request.
* storeboolean or nullOptionalDefaults to false  
Response storage flag. Note: Currently only supports false or null values.
* streamboolean or nullOptionalDefaults to false  
Enable streaming mode to receive response data as server-sent events.
* temperaturenumber or nullOptionalDefaults to 1  
Range: 0 - 2  
Controls randomness in the response generation. Range: 0 to 2\. Lower values produce more deterministic outputs, higher values increase variety and creativity.
* textobjectOptional  
Response format configuration. Supports plain text or structured JSON output.  
### Show properties
* tool\_choicestring / object or nullOptional  
Controls which (if any) tool is called by the model.`none` means the model will not call any tool and instead generates a message.`auto` means the model can pick between generating a message or calling one or more tools.`required` means the model must call one or more tools. Specifying a particular tool via `{"type": "function", "function": {"name": "my_function"}}` forces the model to call that tool.  
`none` is the default when no tools are present. `auto` is the default if tools are present.  
### Show possible types
* toolsarray or nullOptional  
List of tools available to the model. Currently supports function definitions only. Maximum of 128 functions.  
### Show properties
* top\_pnumber or nullOptionalDefaults to 1  
Range: 0 - 1  
Nucleus sampling parameter that controls the cumulative probability cutoff. Range: 0 to 1\. A value of 0.1 restricts sampling to tokens within the top 10% probability mass.
* truncationstring or nullOptionalDefaults to disabled  
Allowed values: `auto, disabled`  
Context truncation strategy. Supported values: `auto` or `disabled`.
* userstringOptional  
Optional identifier for tracking end-user requests. Useful for usage monitoring and compliance.

### 

[Response Object](https://console.groq.com/docs/api-reference#responses-create-returns)

* backgroundboolean  
Whether the response was generated in the background.
* created\_atinteger  
The Unix timestamp (in seconds) of when the response was created.
* errorobject or null  
An error object if the response failed.  
### Show properties
* idstring  
A unique identifier for the response.
* incomplete\_detailsobject or null  
Details about why the response is incomplete.  
### Show properties
* instructionsstring or null  
The system instructions used for the response.
* max\_output\_tokensinteger or null  
The maximum number of tokens configured for the response.
* max\_tool\_callsinteger or null  
The maximum number of tool calls allowed.
* metadataobject or null  
Metadata attached to the response.
* modelstring  
The model used for the response.
* objectstring  
Allowed values: `response`  
The object type, which is always `response`.
* outputarray  
An array of content items generated by the model.  
### Show possible types
* parallel\_tool\_callsboolean  
Whether the model can run tool calls in parallel.
* previous\_response\_idstring or null  
Not supported. Always null.
* reasoningobject or null  
Configuration options for reasoning models.  
### Show properties
* service\_tierstring  
Allowed values: `auto, default, flex`  
The service tier used for processing.
* statusstring  
Allowed values: `completed, failed, in_progress, incomplete`  
The status of the response generation. One of `completed`, `failed`, `in_progress`, or `incomplete`.
* storeboolean  
Whether the response was stored.
* temperaturenumber  
The sampling temperature used.
* textobject  
Text format configuration used for the response.  
### Show properties
* tool\_choicestring / object or null  
Controls which (if any) tool is called by the model.`none` means the model will not call any tool and instead generates a message.`auto` means the model can pick between generating a message or calling one or more tools.`required` means the model must call one or more tools. Specifying a particular tool via `{"type": "function", "function": {"name": "my_function"}}` forces the model to call that tool.  
`none` is the default when no tools are present. `auto` is the default if tools are present.  
### Show possible types
* toolsarray  
The tools that were available to the model.  
### Show properties
* top\_logprobsinteger  
The number of top log probabilities returned.
* top\_pnumber  
The nucleus sampling parameter used.
* truncationstring  
Allowed values: `auto, disabled`  
The truncation strategy used.
* usageobject  
Usage statistics for the response request.  
### Show properties
* userstring or null  
The user identifier.

Example request

```
curl https://api.groq.com/openai/v1/responses -s \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $GROQ_API_KEY" \
-d '{
  "model": "openai/gpt-oss-120b",
  "input": "Tell me a three sentence bedtime story about a unicorn."
}'
```

Example Response

```
{
  "id": "resp_01k1x6w9ane6d8rfxm05cb45yk",
  "object": "response",
  "status": "completed",
  "created_at": 1754400695,
  "output": [
    {
      "type": "message",
      "id": "msg_01k1x6w9ane6eb0650crhawwyy",
      "status": "completed",
      "role": "assistant",
      "content": [
        {
          "type": "output_text",
          "text": "When the stars blinked awake, Luna the unicorn curled her mane and whispered wishes to the sleeping pine trees. She galloped through a field of moonlit daisies, gathering dew like tiny silver pearls. With a gentle sigh, she tucked her hooves beneath a silver cloud so the world slept softly, dreaming of her gentle hooves until the morning.",
          "annotations": []
        }
      ]
    }
  ],
  "previous_response_id": null,
  "model": "llama-3.3-70b-versatile",
  "reasoning": {
    "effort": null,
    "summary": null
  },
  "max_output_tokens": null,
  "instructions": null,
  "text": {
    "format": {
      "type": "text"
    }
  },
  "tools": [],
  "tool_choice": "auto",
  "truncation": "disabled",
  "metadata": {},
  "temperature": 1,
  "top_p": 1,
  "user": null,
  "service_tier": "default",
  "error": null,
  "incomplete_details": null,
  "usage": {
    "input_tokens": 82,
    "input_tokens_details": {
      "cached_tokens": 0
    },
    "output_tokens": 266,
    "output_tokens_details": {
      "reasoning_tokens": 0
    },
    "total_tokens": 348
  },
  "parallel_tool_calls": true,
  "store": false
}
```

[Audio](https://console.groq.com/docs/api-reference#audio)

[Create transcription](https://console.groq.com/docs/api-reference#audio-transcription)

POSThttps://api.groq.com/openai/v1/audio/transcriptions

Transcribes audio into the input language.

### 

[Request Body](https://console.groq.com/docs/api-reference#audio-transcription-request-body)

* modelstringRequired  
ID of the model to use. `whisper-large-v3` and `whisper-large-v3-turbo` are currently available.
* filestringOptional  
The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm. Either a file or a URL must be provided. Note that the file field is not supported in Batch API requests.
* languagestringOptional  
The language of the input audio. Supplying the input language in [ISO-639-1](https://en.wikipedia.org/wiki/List%5Fof%5FISO%5F639-1%5Fcodes) format will improve accuracy and latency.
* promptstringOptional  
An optional text to guide the model's style or continue a previous audio segment. The [prompt](https://console.groq.com/docs/speech-text) should match the audio language.
* response\_formatstringOptionalDefaults to json  
Allowed values: `json, text, verbose_json`  
The format of the transcript output, in one of these options: `json`, `text`, or `verbose_json`.
* temperaturenumberOptionalDefaults to 0  
The sampling temperature, between 0 and 1\. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log%5Fprobability) to automatically increase the temperature until certain thresholds are hit.
* timestamp\_granularities\[\]arrayOptionalDefaults to segment  
The timestamp granularities to populate for this transcription. `response_format` must be set `verbose_json` to use timestamp granularities. Either or both of these options are supported: `word`, or `segment`. Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency.
* urlstringOptional  
The audio URL to translate/transcribe (supports Base64URL). Either a file or a URL must be provided. For Batch API requests, the URL field is required since the file field is not supported.

### 

[Response Object](https://console.groq.com/docs/api-reference#audio-transcription-returns)

* textstring  
The transcribed text.

curl

```
curl https://api.groq.com/openai/v1/audio/transcriptions \
  -H "Authorization: Bearer $GROQ_API_KEY" \
  -H "Content-Type: multipart/form-data" \
  -F file="@./sample_audio.m4a" \
  -F model="whisper-large-v3"
```

```
import fs from "fs";
import Groq from "groq-sdk";

const groq = new Groq();
async function main() {
  const transcription = await groq.audio.transcriptions.create({
    file: fs.createReadStream("sample_audio.m4a"),
    model: "whisper-large-v3",
    prompt: "Specify context or spelling", // Optional
    response_format: "json", // Optional
    language: "en", // Optional
    temperature: 0.0, // Optional
  });
  console.log(transcription.text);
}
main();
```

```
import os
from groq import Groq

client = Groq()
filename = os.path.dirname(__file__) + "/sample_audio.m4a"

with open(filename, "rb") as file:
    transcription = client.audio.transcriptions.create(
      file=(filename, file.read()),
      model="whisper-large-v3",
      prompt="Specify context or spelling",  # Optional
      response_format="json",  # Optional
      language="en",  # Optional
      temperature=0.0  # Optional
    )
    print(transcription.text)
```

Example Response

```
{
  "text": "Your transcribed text appears here...",
  "x_groq": {
    "id": "req_unique_id"
  }
}
```

[Create translation](https://console.groq.com/docs/api-reference#audio-translation)

POSThttps://api.groq.com/openai/v1/audio/translations

Translates audio into English.

### 

[Request Body](https://console.groq.com/docs/api-reference#audio-translation-request-body)

* modelstringRequired  
ID of the model to use. `whisper-large-v3` and `whisper-large-v3-turbo` are currently available.
* filestringOptional  
The audio file object (not file name) translate, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.
* promptstringOptional  
An optional text to guide the model's style or continue a previous audio segment. The [prompt](https://console.groq.com/docs/guides/speech-to-text/prompting) should be in English.
* response\_formatstringOptionalDefaults to json  
Allowed values: `json, text, verbose_json`  
The format of the transcript output, in one of these options: `json`, `text`, or `verbose_json`.
* temperaturenumberOptionalDefaults to 0  
The sampling temperature, between 0 and 1\. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log%5Fprobability) to automatically increase the temperature until certain thresholds are hit.
* urlstringOptional  
The audio URL to translate/transcribe (supports Base64URL). Either file or url must be provided. When using the Batch API only url is supported.

### 

[Response Object](https://console.groq.com/docs/api-reference#audio-translation-returns)

* textstring

curl

```
curl https://api.groq.com/openai/v1/audio/translations \
  -H "Authorization: Bearer $GROQ_API_KEY" \
  -H "Content-Type: multipart/form-data" \
  -F file="@./sample_audio.m4a" \
  -F model="whisper-large-v3"
```

```
// Default
import fs from "fs";
import Groq from "groq-sdk";

const groq = new Groq();
async function main() {
  const translation = await groq.audio.translations.create({
    file: fs.createReadStream("sample_audio.m4a"),
    model: "whisper-large-v3",
    prompt: "Specify context or spelling", // Optional
    response_format: "json", // Optional
    temperature: 0.0, // Optional
  });
  console.log(translation.text);
}
main();
```

```
# Default
import os
from groq import Groq

client = Groq()
filename = os.path.dirname(__file__) + "/sample_audio.m4a"

with open(filename, "rb") as file:
    translation = client.audio.translations.create(
      file=(filename, file.read()),
      model="whisper-large-v3",
      prompt="Specify context or spelling",  # Optional
      response_format="json",  # Optional
      temperature=0.0  # Optional
    )
    print(translation.text)
```

Example Response

```
{
  "text": "Your translated text appears here...",
  "x_groq": {
    "id": "req_unique_id"
  }
}
```

[Create speech](https://console.groq.com/docs/api-reference#audio-speech)

POSThttps://api.groq.com/openai/v1/audio/speech

Generates audio from the input text.

### 

[Request Body](https://console.groq.com/docs/api-reference#audio-speech-request-body)

* inputstringRequired  
The text to generate audio for.
* modelstringRequired  
One of the [available TTS models](https://console.groq.com/docs/text-to-speech).
* voicestringRequired  
The voice to use when generating the audio. List of voices can be found [here](https://console.groq.com/docs/text-to-speech).
* response\_formatstringOptionalDefaults to mp3  
Allowed values: `flac, mp3, mulaw, ogg, wav`  
The format of the generated audio. Supported formats are `flac, mp3, mulaw, ogg, wav`.
* sample\_rateintegerOptionalDefaults to 48000  
Allowed values: `8000, 16000, 22050, 24000, 32000, 44100, 48000`  
The sample rate for generated audio
* speednumberOptionalDefaults to 1  
Range: 0.5 - 5  
The speed of the generated audio.

### 

[Returns](https://console.groq.com/docs/api-reference#audio-speech-returns)

Returns an audio file in `wav` format.

curl

```
curl https://api.groq.com/openai/v1/audio/speech \
  -H "Authorization: Bearer $GROQ_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "playai-tts",
    "input": "I love building and shipping new features for our users!",
    "voice": "Fritz-PlayAI",
    "response_format": "wav"
  }'
```

```
import fs from "fs";
import path from "path";
import Groq from 'groq-sdk';

const groq = new Groq({
  apiKey: process.env.GROQ_API_KEY
});

const speechFilePath = "speech.wav";
const model = "playai-tts";
const voice = "Fritz-PlayAI";
const text = "I love building and shipping new features for our users!";
const responseFormat = "wav";

async function main() {
  const response = await groq.audio.speech.create({
    model: model,
    voice: voice,
    input: text,
    response_format: responseFormat
  });

  const buffer = Buffer.from(await response.arrayBuffer());
  await fs.promises.writeFile(speechFilePath, buffer);
}

main();
```

```
import os
from groq import Groq

client = Groq(api_key=os.environ.get("GROQ_API_KEY"))

speech_file_path = "speech.wav"
model = "playai-tts"
voice = "Fritz-PlayAI"
text = "I love building and shipping new features for our users!"
response_format = "wav"

response = client.audio.speech.create(
    model=model,
    voice=voice,
    input=text,
    response_format=response_format
)

response.write_to_file(speech_file_path)
```

Example Response

```
"string"
```

[Models](https://console.groq.com/docs/api-reference#models)

[List models](https://console.groq.com/docs/api-reference#models-list)

GEThttps://api.groq.com/openai/v1/models

List all available [models](https://console.groq.com/docs/models).

### 

[Response Object](https://console.groq.com/docs/api-reference#models-list-returns)

* dataarray  
### Show properties
* objectstring  
Allowed values: `list`

curl

```
curl https://api.groq.com/openai/v1/models \
-H "Authorization: Bearer $GROQ_API_KEY"
```

```
import Groq from "groq-sdk";

const groq = new Groq({ apiKey: process.env.GROQ_API_KEY });

async function main() {
  const models = await groq.models.list();
  console.log(models);
}

main();
```

```
import os
from groq import Groq

client = Groq(
    # This is the default and can be omitted
    api_key=os.environ.get("GROQ_API_KEY"),
)

models = client.models.list()

print(models)
```

Example Response

```
{
  "object": "list",
  "data": [
    {
      "id": "gemma2-9b-it",
      "object": "model",
      "created": 1693721698,
      "owned_by": "Google",
      "active": true,
      "context_window": 8192,
      "public_apps": null
    },
    {
      "id": "llama3-8b-8192",
      "object": "model",
      "created": 1693721698,
      "owned_by": "Meta",
      "active": true,
      "context_window": 8192,
      "public_apps": null
    },
    {
      "id": "llama3-70b-8192",
      "object": "model",
      "created": 1693721698,
      "owned_by": "Meta",
      "active": true,
      "context_window": 8192,
      "public_apps": null
    },
    {
      "id": "whisper-large-v3-turbo",
      "object": "model",
      "created": 1728413088,
      "owned_by": "OpenAI",
      "active": true,
      "context_window": 448,
      "public_apps": null
    },
    {
      "id": "whisper-large-v3",
      "object": "model",
      "created": 1693721698,
      "owned_by": "OpenAI",
      "active": true,
      "context_window": 448,
      "public_apps": null
    },
    {
      "id": "llama-guard-3-8b",
      "object": "model",
      "created": 1693721698,
      "owned_by": "Meta",
      "active": true,
      "context_window": 8192,
      "public_apps": null
    },
    {
      "id": "distil-whisper-large-v3-en",
      "object": "model",
      "created": 1693721698,
      "owned_by": "Hugging Face",
      "active": true,
      "context_window": 448,
      "public_apps": null
    },
    {
      "id": "llama-3.1-8b-instant",
      "object": "model",
      "created": 1693721698,
      "owned_by": "Meta",
      "active": true,
      "context_window": 131072,
      "public_apps": null
    }
  ]
}
```

[Retrieve model](https://console.groq.com/docs/api-reference#models-retrieve)

GEThttps://api.groq.com/openai/v1/models/{model}

Get detailed information about a [model](https://console.groq.com/docs/models).

### 

[Response Object](https://console.groq.com/docs/api-reference#models-retrieve-returns)

* createdinteger  
The Unix timestamp (in seconds) when the model was created.
* idstring  
The model identifier, which can be referenced in the API endpoints.
* objectstring  
Allowed values: `model`  
The object type, which is always "model".
* owned\_bystring  
The organization that owns the model.

curl

```
curl https://api.groq.com/openai/v1/models/llama-3.3-70b-versatile \
-H "Authorization: Bearer $GROQ_API_KEY"
```

```
import Groq from "groq-sdk";

const groq = new Groq({ apiKey: process.env.GROQ_API_KEY });

async function main() {
  const model = await groq.models.retrieve("llama-3.3-70b-versatile");
  console.log(model);
}

main();
```

```
import os
from groq import Groq

client = Groq(
    # This is the default and can be omitted
    api_key=os.environ.get("GROQ_API_KEY"),
)

model = client.models.retrieve("llama-3.3-70b-versatile")

print(model)
```

Example Response

```
{
  "id": "llama3-8b-8192",
  "object": "model",
  "created": 1693721698,
  "owned_by": "Meta",
  "active": true,
  "context_window": 8192,
  "public_apps": null,
  "max_completion_tokens": 8192
}
```

[Batches](https://console.groq.com/docs/api-reference#batches)

[Create batch](https://console.groq.com/docs/api-reference#batches-create)

POSThttps://api.groq.com/openai/v1/batches

Creates and executes a batch from an uploaded file of requests. [Learn more](https://console.groq.com/docs/batch).

### 

[Request Body](https://console.groq.com/docs/api-reference#batches-create-request-body)

* completion\_windowstringRequired  
The time frame within which the batch should be processed. Durations from `24h` to `7d` are supported.
* endpointstringRequired  
Allowed values: `/v1/chat/completions`  
The endpoint to be used for all requests in the batch. Currently `/v1/chat/completions` is supported.
* input\_file\_idstringRequired  
The ID of an uploaded file that contains requests for the new batch.  
See [upload file](https://console.groq.com/docs/api-reference#files-upload) for how to upload a file.  
Your input file must be formatted as a [JSONL file](https://console.groq.com/docs/batch), and must be uploaded with the purpose `batch`. The file can be up to 100 MB in size.
* metadataobject or nullOptional  
Optional custom metadata for the batch.

### 

[Response Object](https://console.groq.com/docs/api-reference#batches-create-returns)

* cancelled\_atinteger  
The Unix timestamp (in seconds) for when the batch was cancelled.
* cancelling\_atinteger  
The Unix timestamp (in seconds) for when the batch started cancelling.
* completed\_atinteger  
The Unix timestamp (in seconds) for when the batch was completed.
* completion\_windowstring  
The time frame within which the batch should be processed.
* created\_atinteger  
The Unix timestamp (in seconds) for when the batch was created.
* endpointstring  
The API endpoint used by the batch.
* error\_file\_idstring  
The ID of the file containing the outputs of requests with errors.
* errorsobject  
### Show properties
* expired\_atinteger  
The Unix timestamp (in seconds) for when the batch expired.
* expires\_atinteger  
The Unix timestamp (in seconds) for when the batch will expire.
* failed\_atinteger  
The Unix timestamp (in seconds) for when the batch failed.
* finalizing\_atinteger  
The Unix timestamp (in seconds) for when the batch started finalizing.
* idstring
* in\_progress\_atinteger  
The Unix timestamp (in seconds) for when the batch started processing.
* input\_file\_idstring  
The ID of the input file for the batch.
* metadataobject or null  
Set of key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format.
* objectstring  
Allowed values: `batch`  
The object type, which is always `batch`.
* output\_file\_idstring  
The ID of the file containing the outputs of successfully executed requests.
* request\_countsobject  
The request counts for different statuses within the batch.  
### Show properties
* statusstring  
Allowed values: `validating, failed, in_progress, finalizing, completed, expired, cancelling, cancelled`  
The current status of the batch.

curl

```
curl https://api.groq.com/openai/v1/batches \
  -H "Authorization: Bearer $GROQ_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "input_file_id": "file_01jh6x76wtemjr74t1fh0faj5t",
    "endpoint": "/v1/chat/completions",
    "completion_window": "24h"
  }'
```

```
import Groq from 'groq-sdk';

const client = new Groq({
  apiKey: process.env['GROQ_API_KEY'], // This is the default and can be omitted
});

async function main() {
  const batch = await client.batches.create({
    completion_window: "24h",
    endpoint: "/v1/chat/completions",
    input_file_id: "file_01jh6x76wtemjr74t1fh0faj5t",
  });
  console.log(batch.id);
}

main();
```

```
import os
from groq import Groq

client = Groq(
    api_key=os.environ.get("GROQ_API_KEY"),  # This is the default and can be omitted
)
batch = client.batches.create(
    completion_window="24h",
    endpoint="/v1/chat/completions",
    input_file_id="file_01jh6x76wtemjr74t1fh0faj5t",
)
print(batch.id)
```

Example Response

```
{
  "id": "batch_01jh6xa7reempvjyh6n3yst2zw",
  "object": "batch",
  "endpoint": "/v1/chat/completions",
  "errors": null,
  "input_file_id": "file_01jh6x76wtemjr74t1fh0faj5t",
  "completion_window": "24h",
  "status": "validating",
  "output_file_id": null,
  "error_file_id": null,
  "finalizing_at": null,
  "failed_at": null,
  "expired_at": null,
  "cancelled_at": null,
  "request_counts": {
    "total": 0,
    "completed": 0,
    "failed": 0
  },
  "metadata": null,
  "created_at": 1736472600,
  "expires_at": 1736559000,
  "cancelling_at": null,
  "completed_at": null,
  "in_progress_at": null
}
```

[Retrieve batch](https://console.groq.com/docs/api-reference#batches-retrieve)

GEThttps://api.groq.com/openai/v1/batches/{batch\_id}

Retrieves a batch.

### 

[Response Object](https://console.groq.com/docs/api-reference#batches-retrieve-returns)

* cancelled\_atinteger  
The Unix timestamp (in seconds) for when the batch was cancelled.
* cancelling\_atinteger  
The Unix timestamp (in seconds) for when the batch started cancelling.
* completed\_atinteger  
The Unix timestamp (in seconds) for when the batch was completed.
* completion\_windowstring  
The time frame within which the batch should be processed.
* created\_atinteger  
The Unix timestamp (in seconds) for when the batch was created.
* endpointstring  
The API endpoint used by the batch.
* error\_file\_idstring  
The ID of the file containing the outputs of requests with errors.
* errorsobject  
### Show properties
* expired\_atinteger  
The Unix timestamp (in seconds) for when the batch expired.
* expires\_atinteger  
The Unix timestamp (in seconds) for when the batch will expire.
* failed\_atinteger  
The Unix timestamp (in seconds) for when the batch failed.
* finalizing\_atinteger  
The Unix timestamp (in seconds) for when the batch started finalizing.
* idstring
* in\_progress\_atinteger  
The Unix timestamp (in seconds) for when the batch started processing.
* input\_file\_idstring  
The ID of the input file for the batch.
* metadataobject or null  
Set of key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format.
* objectstring  
Allowed values: `batch`  
The object type, which is always `batch`.
* output\_file\_idstring  
The ID of the file containing the outputs of successfully executed requests.
* request\_countsobject  
The request counts for different statuses within the batch.  
### Show properties
* statusstring  
Allowed values: `validating, failed, in_progress, finalizing, completed, expired, cancelling, cancelled`  
The current status of the batch.

curl

```
curl https://api.groq.com/openai/v1/batches/batch_01jh6xa7reempvjyh6n3yst2zw \
  -H "Authorization: Bearer $GROQ_API_KEY" \
  -H "Content-Type: application/json"
```

```
import Groq from 'groq-sdk';

const client = new Groq({
  apiKey: process.env['GROQ_API_KEY'], // This is the default and can be omitted
});

async function main() {
  const batch = await client.batches.retrieve("batch_01jh6xa7reempvjyh6n3yst2zw");
  console.log(batch.id);
}

main();
```

```
import os
from groq import Groq

client = Groq(
    api_key=os.environ.get("GROQ_API_KEY"),  # This is the default and can be omitted
)
batch = client.batches.retrieve(
    "batch_01jh6xa7reempvjyh6n3yst2zw",
)
print(batch.id)
```

Example Response

```
{
  "id": "batch_01jh6xa7reempvjyh6n3yst2zw",
  "object": "batch",
  "endpoint": "/v1/chat/completions",
  "errors": null,
  "input_file_id": "file_01jh6x76wtemjr74t1fh0faj5t",
  "completion_window": "24h",
  "status": "validating",
  "output_file_id": null,
  "error_file_id": null,
  "finalizing_at": null,
  "failed_at": null,
  "expired_at": null,
  "cancelled_at": null,
  "request_counts": {
    "total": 0,
    "completed": 0,
    "failed": 0
  },
  "metadata": null,
  "created_at": 1736472600,
  "expires_at": 1736559000,
  "cancelling_at": null,
  "completed_at": null,
  "in_progress_at": null
}
```

[List batches](https://console.groq.com/docs/api-reference#batches-list)

GEThttps://api.groq.com/openai/v1/batches

List your organization's batches.

### 

[Response Object](https://console.groq.com/docs/api-reference#batches-list-returns)

* dataarray  
### Show properties
* objectstring  
Allowed values: `list`

curl

```
curl https://api.groq.com/openai/v1/batches \
  -H "Authorization: Bearer $GROQ_API_KEY" \
  -H "Content-Type: application/json"
```

```
import Groq from 'groq-sdk';

const client = new Groq({
  apiKey: process.env['GROQ_API_KEY'], // This is the default and can be omitted
});

async function main() {
  const batchList = await client.batches.list();
  console.log(batchList.data);
}

main();
```

```
import os
from groq import Groq

client = Groq(
    api_key=os.environ.get("GROQ_API_KEY"),  # This is the default and can be omitted
)
batch_list = client.batches.list()
print(batch_list.data)
```

Example Response

```
{
  "object": "list",
  "data": [
    {
      "id": "batch_01jh6xa7reempvjyh6n3yst2zw",
      "object": "batch",
      "endpoint": "/v1/chat/completions",
      "errors": null,
      "input_file_id": "file_01jh6x76wtemjr74t1fh0faj5t",
      "completion_window": "24h",
      "status": "validating",
      "output_file_id": null,
      "error_file_id": null,
      "finalizing_at": null,
      "failed_at": null,
      "expired_at": null,
      "cancelled_at": null,
      "request_counts": {
        "total": 0,
        "completed": 0,
        "failed": 0
      },
      "metadata": null,
      "created_at": 1736472600,
      "expires_at": 1736559000,
      "cancelling_at": null,
      "completed_at": null,
      "in_progress_at": null
    }
  ]
}
```

[Cancel batch](https://console.groq.com/docs/api-reference#batches-cancel)

POSThttps://api.groq.com/openai/v1/batches/{batch\_id}/cancel

Cancels a batch.

### 

[Response Object](https://console.groq.com/docs/api-reference#batches-cancel-returns)

* cancelled\_atinteger  
The Unix timestamp (in seconds) for when the batch was cancelled.
* cancelling\_atinteger  
The Unix timestamp (in seconds) for when the batch started cancelling.
* completed\_atinteger  
The Unix timestamp (in seconds) for when the batch was completed.
* completion\_windowstring  
The time frame within which the batch should be processed.
* created\_atinteger  
The Unix timestamp (in seconds) for when the batch was created.
* endpointstring  
The API endpoint used by the batch.
* error\_file\_idstring  
The ID of the file containing the outputs of requests with errors.
* errorsobject  
### Show properties
* expired\_atinteger  
The Unix timestamp (in seconds) for when the batch expired.
* expires\_atinteger  
The Unix timestamp (in seconds) for when the batch will expire.
* failed\_atinteger  
The Unix timestamp (in seconds) for when the batch failed.
* finalizing\_atinteger  
The Unix timestamp (in seconds) for when the batch started finalizing.
* idstring
* in\_progress\_atinteger  
The Unix timestamp (in seconds) for when the batch started processing.
* input\_file\_idstring  
The ID of the input file for the batch.
* metadataobject or null  
Set of key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format.
* objectstring  
Allowed values: `batch`  
The object type, which is always `batch`.
* output\_file\_idstring  
The ID of the file containing the outputs of successfully executed requests.
* request\_countsobject  
The request counts for different statuses within the batch.  
### Show properties
* statusstring  
Allowed values: `validating, failed, in_progress, finalizing, completed, expired, cancelling, cancelled`  
The current status of the batch.

curl

```
curl -X POST https://api.groq.com/openai/v1/batches/batch_01jh6xa7reempvjyh6n3yst2zw/cancel \
  -H "Authorization: Bearer $GROQ_API_KEY" \
  -H "Content-Type: application/json"
```

```
import Groq from 'groq-sdk';

const client = new Groq({
  apiKey: process.env['GROQ_API_KEY'], // This is the default and can be omitted
});

async function main() {
  const batch = await client.batches.cancel("batch_01jh6xa7reempvjyh6n3yst2zw");
  console.log(batch.id);
}

main();
```

```
import os
from groq import Groq

client = Groq(
    api_key=os.environ.get("GROQ_API_KEY"),  # This is the default and can be omitted
)
batch = client.batches.cancel(
    "batch_01jh6xa7reempvjyh6n3yst2zw",
)
print(batch.id)
```

Example Response

```
{
  "id": "batch_01jh6xa7reempvjyh6n3yst2zw",
  "object": "batch",
  "endpoint": "/v1/chat/completions",
  "errors": null,
  "input_file_id": "file_01jh6x76wtemjr74t1fh0faj5t",
  "completion_window": "24h",
  "status": "cancelling",
  "output_file_id": null,
  "error_file_id": null,
  "finalizing_at": null,
  "failed_at": null,
  "expired_at": null,
  "cancelled_at": null,
  "request_counts": {
    "total": 0,
    "completed": 0,
    "failed": 0
  },
  "metadata": null,
  "created_at": 1736472600,
  "expires_at": 1736559000,
  "cancelling_at": null,
  "completed_at": null,
  "in_progress_at": null
}
```

[Files](https://console.groq.com/docs/api-reference#files)

[Upload file](https://console.groq.com/docs/api-reference#files-upload)

POSThttps://api.groq.com/openai/v1/files

Upload a file that can be used across various endpoints.

The Batch API only supports `.jsonl` files up to 100 MB in size. The input also has a specific required [format](https://console.groq.com/docs/batch).

Please contact us if you need to increase these storage limits.

### 

[Request Body](https://console.groq.com/docs/api-reference#files-upload-request-body)

* filestringRequired  
The File object (not file name) to be uploaded.
* purposestringRequired  
Allowed values: `batch`  
The intended purpose of the uploaded file. Use "batch" for [Batch API](https://console.groq.com/docs/api-reference#batches).

### 

[Response Object](https://console.groq.com/docs/api-reference#files-upload-returns)

* bytesinteger  
The size of the file, in bytes.
* created\_atinteger  
The Unix timestamp (in seconds) for when the file was created.
* filenamestring  
The name of the file.
* idstring  
The file identifier, which can be referenced in the API endpoints.
* objectstring  
Allowed values: `file`  
The object type, which is always `file`.
* purposestring  
Allowed values: `batch, batch_output`  
The intended purpose of the file. Supported values are `batch`, and `batch_output`.

curl

```
curl https://api.groq.com/openai/v1/files \
  -H "Authorization: Bearer $GROQ_API_KEY" \
  -F purpose="batch" \
  -F "file=@batch_file.jsonl"
```

```
import Groq from 'groq-sdk';

const client = new Groq({
  apiKey: process.env['GROQ_API_KEY'], // This is the default and can be omitted
});

const fileContent = '{"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "llama-3.1-8b-instant", "messages": [{"role": "user", "content": "Explain the importance of fast language models"}]}}\n';

async function main() {
  const blob = new Blob([fileContent]);
  const file = new File([blob], 'batch.jsonl');

  const createdFile = await client.files.create({ file: file, purpose: 'batch' });
  console.log(createdFile.id);
}

main();
```

```
import os
import requests # pip install requests first!

def upload_file_to_groq(api_key, file_path):
    url = "https://api.groq.com/openai/v1/files"

    headers = {
        "Authorization": f"Bearer {api_key}"
    }

    # Prepare the file and form data
    files = {
        "file": ("batch_file.jsonl", open(file_path, "rb"))
    }

    data = {
        "purpose": "batch"
    }

    # Make the POST request
    response = requests.post(url, headers=headers, files=files, data=data)

    return response.json()

# Usage example
api_key = os.environ.get("GROQ_API_KEY")
file_path = "batch_file.jsonl"  # Path to your JSONL file

try:
    result = upload_file_to_groq(api_key, file_path)
    print(result)
except Exception as e:
    print(f"Error: {e}")
```

Example Response

```
{
  "id": "file_01jh6x76wtemjr74t1fh0faj5t",
  "object": "file",
  "bytes": 966,
  "created_at": 1736472501,
  "filename": "batch_file.jsonl",
  "purpose": "batch"
}
```

[List files](https://console.groq.com/docs/api-reference#files-list)

GEThttps://api.groq.com/openai/v1/files

Returns a list of files.

### 

[Response Object](https://console.groq.com/docs/api-reference#files-list-returns)

* dataarray  
### Show properties
* objectstring  
Allowed values: `list`

curl

```
curl https://api.groq.com/openai/v1/files \
  -H "Authorization: Bearer $GROQ_API_KEY" \
  -H "Content-Type: application/json"
```

```
import Groq from 'groq-sdk';

const client = new Groq({
  apiKey: process.env['GROQ_API_KEY'], // This is the default and can be omitted
});

async function main() {
  const fileList = await client.files.list();
  console.log(fileList.data);
}

main();
```

```
import os
from groq import Groq

client = Groq(
    api_key=os.environ.get("GROQ_API_KEY"),  # This is the default and can be omitted
)
file_list = client.files.list()
print(file_list.data)
```

Example Response

```
{
  "object": "list",
  "data": [
    {
      "id": "file_01jh6x76wtemjr74t1fh0faj5t",
      "object": "file",
      "bytes": 966,
      "created_at": 1736472501,
      "filename": "batch_file.jsonl",
      "purpose": "batch"
    }
  ]
}
```

[Delete file](https://console.groq.com/docs/api-reference#files-delete)

DELETEhttps://api.groq.com/openai/v1/files/{file\_id}

Delete a file.

### 

[Response Object](https://console.groq.com/docs/api-reference#files-delete-returns)

* deletedboolean
* idstring
* objectstring  
Allowed values: `file`

curl

```
curl -X DELETE https://api.groq.com/openai/v1/files/file_01jh6x76wtemjr74t1fh0faj5t \
  -H "Authorization: Bearer $GROQ_API_KEY" \
  -H "Content-Type: application/json"
```

```
import Groq from 'groq-sdk';

const client = new Groq({
  apiKey: process.env['GROQ_API_KEY'], // This is the default and can be omitted
});

async function main() {
  const fileDelete = await client.files.delete("file_01jh6x76wtemjr74t1fh0faj5t");
  console.log(fileDelete);
}

main();
```

```
import os
from groq import Groq

client = Groq(
    api_key=os.environ.get("GROQ_API_KEY"),  # This is the default and can be omitted
)
file_delete = client.files.delete(
    "file_01jh6x76wtemjr74t1fh0faj5t",
)
print(file_delete)
```

Example Response

```
{
  "id": "file_01jh6x76wtemjr74t1fh0faj5t",
  "object": "file",
  "deleted": true
}
```

[Retrieve file](https://console.groq.com/docs/api-reference#files-retrieve)

GEThttps://api.groq.com/openai/v1/files/{file\_id}

Returns information about a file.

### 

[Response Object](https://console.groq.com/docs/api-reference#files-retrieve-returns)

* bytesinteger  
The size of the file, in bytes.
* created\_atinteger  
The Unix timestamp (in seconds) for when the file was created.
* filenamestring  
The name of the file.
* idstring  
The file identifier, which can be referenced in the API endpoints.
* objectstring  
Allowed values: `file`  
The object type, which is always `file`.
* purposestring  
Allowed values: `batch, batch_output`  
The intended purpose of the file. Supported values are `batch`, and `batch_output`.

curl

```
curl https://api.groq.com/openai/v1/files/file_01jh6x76wtemjr74t1fh0faj5t \
  -H "Authorization: Bearer $GROQ_API_KEY" \
  -H "Content-Type: application/json"
```

```
import Groq from 'groq-sdk';

const client = new Groq({
  apiKey: process.env['GROQ_API_KEY'], // This is the default and can be omitted
});

async function main() {
    const file = await client.files.info('file_01jh6x76wtemjr74t1fh0faj5t');
    console.log(file);
}

main();
```

```
import os
from groq import Groq

client = Groq(
    api_key=os.environ.get("GROQ_API_KEY"),  # This is the default and can be omitted
)
file = client.files.info(
    "file_01jh6x76wtemjr74t1fh0faj5t",
)
print(file)
```

Example Response

```
{
  "id": "file_01jh6x76wtemjr74t1fh0faj5t",
  "object": "file",
  "bytes": 966,
  "created_at": 1736472501,
  "filename": "batch_file.jsonl",
  "purpose": "batch"
}
```

[Download file](https://console.groq.com/docs/api-reference#files-download)

GEThttps://api.groq.com/openai/v1/files/{file\_id}/content

Returns the contents of the specified file.

### 

[Returns](https://console.groq.com/docs/api-reference#files-download-returns)

The file content

curl

```
curl https://api.groq.com/openai/v1/files/file_01jh6x76wtemjr74t1fh0faj5t/content \
  -H "Authorization: Bearer $GROQ_API_KEY" \
  -H "Content-Type: application/json"
```

```
import Groq from 'groq-sdk';

const client = new Groq({
  apiKey: process.env['GROQ_API_KEY'], // This is the default and can be omitted
});

async function main() {
    const response = await client.files.content('file_01jh6x76wtemjr74t1fh0faj5t');
    console.log(response);
}

main();
```

```
import os
from groq import Groq

client = Groq(
    api_key=os.environ.get("GROQ_API_KEY"),  # This is the default and can be omitted
)
response = client.files.content(
    "file_01jh6x76wtemjr74t1fh0faj5t",
)
print(response)
```

Example Response

```
"string"
```

[Fine Tuning](https://console.groq.com/docs/api-reference#fine-tuning)

[List fine tunings](https://console.groq.com/docs/api-reference#fine-tuning-list)

GEThttps://api.groq.com/v1/fine\_tunings

Lists all previously created fine tunings. This endpoint is in closed beta. [Contact us](https://groq.com/contact) for more information.

### 

[Response Object](https://console.groq.com/docs/api-reference#fine-tuning-list-returns)

* dataarray  
### Show properties
* objectstring

curl

```
curl https://api.groq.com/v1/fine_tunings -s \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $GROQ_API_KEY"
```

```
import Groq from "groq-sdk";

const groq = new Groq({ apiKey: process.env.GROQ_API_KEY });

async function main() {
    const fineTunings = await groq.fine_tunings.list();
    console.log(fineTunings);
}

main();
```

```
import os

from groq import Groq

client = Groq(
    # This is the default and can be omitted
    api_key=os.environ.get("GROQ_API_KEY"),
)

fine_tunings = client.fine_tunings.list()

print(fine_tunings)
```

Example Response

```
{
    "object": "list",
    "data": [
        {
            "id": "string",
            "name": "string",
            "base_model": "string",
            "type": "string",
            "input_file_id": "string",
            "created_at": 0,
            "fine_tuned_model": "string"
        }
    ]
}
```

[Create fine tuning](https://console.groq.com/docs/api-reference#fine-tuning-create)

POSThttps://api.groq.com/v1/fine\_tunings

Creates a new fine tuning for the already uploaded files This endpoint is in closed beta. [Contact us](https://groq.com/contact) for more information.

### 

[Request Body](https://console.groq.com/docs/api-reference#fine-tuning-create-request-body)

* base\_modelstringOptional  
BaseModel is the model that the fine tune was originally trained on.
* input\_file\_idstringOptional  
InputFileID is the id of the file that was uploaded via the /files api.
* namestringOptional  
Name is the given name to a fine tuned model.
* typestringOptional  
Type is the type of fine tuning format such as "lora".

### 

[Response Object](https://console.groq.com/docs/api-reference#fine-tuning-create-returns)

* dataobject  
### Show properties
* idstring
* objectstring

curl

```
curl https://api.groq.com/v1/fine_tunings -s \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $GROQ_API_KEY" \
    -d '{
        "input_file_id": "<file-id>",
        "name": "test-1",
        "type": "lora",
        "base_model": "llama-3.1-8b-instant"
    }'
```

```
import Groq from "groq-sdk";

const groq = new Groq({ apiKey: process.env.GROQ_API_KEY });

async function main() {
    const fineTunings = await groq.fine_tunings.create({
        input_file_id: "<file-id>",
        name: "test-1",
        type: "lora",
        base_model: "llama-3.1-8b-instant"
    });
    console.log(fineTunings);
}

main();
```

```
import os

from groq import Groq

client = Groq(
    # This is the default and can be omitted
    api_key=os.environ.get("GROQ_API_KEY"),
)

fine_tunings = client.fine_tunings.create(
    input_file_id="<file-id>",
    name="test-1",
    type="lora",
    base_model="llama-3.1-8b-instant"
)

print(fine_tunings)
```

Example Response

```
{
    "id": "string",
    "object": "object",
    "data": {
        "id": "string",
        "name": "string",
        "base_model": "string",
        "type": "string",
        "input_file_id": "string",
        "created_at": 0,
        "fine_tuned_model": "string"
    }
}
```

[Get fine tuning](https://console.groq.com/docs/api-reference#fine-tuning-get)

GEThttps://api.groq.com/v1/fine\_tunings/{id}

Retrieves an existing fine tuning by id This endpoint is in closed beta. [Contact us](https://groq.com/contact) for more information.

### 

[Response Object](https://console.groq.com/docs/api-reference#fine-tuning-get-returns)

* dataobject  
### Show properties
* idstring
* objectstring

curl

```
curl https://api.groq.com/v1/fine_tunings/:id -s \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $GROQ_API_KEY"
```

```
import Groq from "groq-sdk";

const groq = new Groq({ apiKey: process.env.GROQ_API_KEY });

async function main() {
    const fineTuning = await groq.fine_tunings.get({id: "<id>"});
    console.log(fineTuning);
}

main();
```

```
import os

from groq import Groq

client = Groq(
    # This is the default and can be omitted
    api_key=os.environ.get("GROQ_API_KEY"),
)

fine_tuning = client.fine_tunings.get(id="<id>")

print(fine_tuning)
```

Example Response

```
{
    "id": "string",
    "object": "object",
    "data": {
        "id": "string",
        "name": "string",
        "base_model": "string",
        "type": "string",
        "input_file_id": "string",
        "created_at": 0,
        "fine_tuned_model": "string"
    }
}
```

[Delete fine tuning](https://console.groq.com/docs/api-reference#fine-tuning-delete)

DELETEhttps://api.groq.com/v1/fine\_tunings/{id}

Deletes an existing fine tuning by id This endpoint is in closed beta. [Contact us](https://groq.com/contact) for more information.

### 

[Response Object](https://console.groq.com/docs/api-reference#fine-tuning-delete-returns)

* deletedboolean
* idstring
* objectstring

curl

```
curl -X DELETE https://api.groq.com/v1/fine_tunings/:id -s \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $GROQ_API_KEY"
```

```
import Groq from "groq-sdk";

const groq = new Groq({ apiKey: process.env.GROQ_API_KEY });

async function main() {
    await groq.fine_tunings.delete({id: "<id>"});
}

main();
```

```
import os

from groq import Groq

client = Groq(
    # This is the default and can be omitted
    api_key=os.environ.get("GROQ_API_KEY"),
)

client.fine_tunings.delete(id="<id>")
```

Example Response

```
{
    "id": "string",
    "object": "fine_tuning",
    "deleted": true
}
```
---
description: Learn how to generate text and have conversations with Groq&#x27;s Chat Completions API, including streaming, JSON mode, and advanced features.
title: Text Generation - GroqDocs
image: https://console.groq.com/og_cloudv5.jpg
---

# Text Generation

Generating text with Groq's Chat Completions API enables you to have natural, conversational interactions with Groq's large language models. It processes a series of messages and generates human-like responses that can be used for various applications including conversational agents, content generation, task automation, and generating structured data outputs like JSON for your applications.

## [Chat Completions](#chat-completions)

Chat completions allow your applications to have dynamic interactions with Groq's models. You can send messages that include user inputs and system instructions, and receive responses that match the conversational context.

  
Chat models can handle both multi-turn discussions (conversations with multiple back-and-forth exchanges) and single-turn tasks where you need just one response.

  
For details about all available parameters, [visit the API reference page.](https://console.groq.com/docs/api-reference#chat-create)

### [Getting Started with Groq SDK](#getting-started-with-groq-sdk)

To start using Groq's Chat Completions API, you'll need to install the [Groq SDK](https://console.groq.com/docs/libraries) and set up your [API key](https://console.groq.com/keys).

PythonJavaScript

shell

```
pip install groq
```

## [Performing a Basic Chat Completion](#performing-a-basic-chat-completion)

The simplest way to use the Chat Completions API is to send a list of messages and receive a single response. Messages are provided in chronological order, with each message containing a role ("system", "user", or "assistant") and content.

Python

```
from groq import Groq

client = Groq()

chat_completion = client.chat.completions.create(
    messages=[
        # Set an optional system message. This sets the behavior of the
        # assistant and can be used to provide specific instructions for
        # how it should behave throughout the conversation.
        {
            "role": "system",
            "content": "You are a helpful assistant."
        },
        # Set a user message for the assistant to respond to.
        {
            "role": "user",
            "content": "Explain the importance of fast language models",
        }
    ],

    # The language model which will generate the completion.
    model="llama-3.3-70b-versatile"
)

# Print the completion returned by the LLM.
print(chat_completion.choices[0].message.content)
```

## [Streaming a Chat Completion](#streaming-a-chat-completion)

For a more responsive user experience, you can stream the model's response in real-time. This allows your application to display the response as it's being generated, rather than waiting for the complete response.

To enable streaming, set the parameter `stream=True`. The completion function will then return an iterator of completion deltas rather than a single, full completion.

Python

```
from groq import Groq

client = Groq()

stream = client.chat.completions.create(
    #
    # Required parameters
    #
    messages=[
        # Set an optional system message. This sets the behavior of the
        # assistant and can be used to provide specific instructions for
        # how it should behave throughout the conversation.
        {
            "role": "system",
            "content": "You are a helpful assistant."
        },
        # Set a user message for the assistant to respond to.
        {
            "role": "user",
            "content": "Explain the importance of fast language models",
        }
    ],

    # The language model which will generate the completion.
    model="llama-3.3-70b-versatile",

    #
    # Optional parameters
    #

    # Controls randomness: lowering results in less random completions.
    # As the temperature approaches zero, the model will become deterministic
    # and repetitive.
    temperature=0.5,

    # The maximum number of tokens to generate. Requests can use up to
    # 2048 tokens shared between prompt and completion.
    max_completion_tokens=1024,

    # Controls diversity via nucleus sampling: 0.5 means half of all
    # likelihood-weighted options are considered.
    top_p=1,

    # A stop sequence is a predefined or user-specified text string that
    # signals an AI to stop generating content, ensuring its responses
    # remain focused and concise. Examples include punctuation marks and
    # markers like "[end]".
    stop=None,

    # If set, partial message deltas will be sent.
    stream=True,
)

# Print the incremental deltas returned by the LLM.
for chunk in stream:
    print(chunk.choices[0].delta.content, end="")
```

## [Performing a Chat Completion with a Stop Sequence](#performing-a-chat-completion-with-a-stop-sequence)

Stop sequences allow you to control where the model should stop generating. When the model encounters any of the specified stop sequences, it will halt generation at that point. This is useful when you need responses to end at specific points.

Python

```
from groq import Groq

client = Groq()

chat_completion = client.chat.completions.create(
    #
    # Required parameters
    #
    messages=[
        # Set an optional system message. This sets the behavior of the
        # assistant and can be used to provide specific instructions for
        # how it should behave throughout the conversation.
        {
            "role": "system",
            "content": "You are a helpful assistant."
        },
        # Set a user message for the assistant to respond to.
        {
            "role": "user",
            "content": "Count to 10.  Your response must begin with \"1, \".  example: 1, 2, 3, ...",
        }
    ],

    # The language model which will generate the completion.
    model="llama-3.3-70b-versatile",

    #
    # Optional parameters
    #

    # Controls randomness: lowering results in less random completions.
    # As the temperature approaches zero, the model will become deterministic
    # and repetitive.
    temperature=0.5,

    # The maximum number of tokens to generate. Requests can use up to
    # 2048 tokens shared between prompt and completion.
    max_completion_tokens=1024,

    # Controls diversity via nucleus sampling: 0.5 means half of all
    # likelihood-weighted options are considered.
    top_p=1,

    # A stop sequence is a predefined or user-specified text string that
    # signals an AI to stop generating content, ensuring its responses
    # remain focused and concise. Examples include punctuation marks and
    # markers like "[end]".
    # For this example, we will use ", 6" so that the llm stops counting at 5.
    # If multiple stop values are needed, an array of string may be passed,
    # stop=[", 6", ", six", ", Six"]
    stop=", 6",

    # If set, partial message deltas will be sent.
    stream=False,
)

# Print the completion returned by the LLM.
print(chat_completion.choices[0].message.content)
```

## [Performing an Async Chat Completion](#performing-an-async-chat-completion)

For applications that need to maintain responsiveness while waiting for completions, you can use the asynchronous client. This lets you make non-blocking API calls using Python's asyncio framework.

Python

```
import asyncio

from groq import AsyncGroq


async def main():
    client = AsyncGroq()

    chat_completion = await client.chat.completions.create(
        #
        # Required parameters
        #
        messages=[
            # Set an optional system message. This sets the behavior of the
            # assistant and can be used to provide specific instructions for
            # how it should behave throughout the conversation.
            {
                "role": "system",
                "content": "You are a helpful assistant."
            },
            # Set a user message for the assistant to respond to.
            {
                "role": "user",
                "content": "Explain the importance of fast language models",
            }
        ],

        # The language model which will generate the completion.
        model="llama-3.3-70b-versatile",

        #
        # Optional parameters
        #

        # Controls randomness: lowering results in less random completions.
        # As the temperature approaches zero, the model will become
        # deterministic and repetitive.
        temperature=0.5,

        # The maximum number of tokens to generate. Requests can use up to
        # 2048 tokens shared between prompt and completion.
        max_completion_tokens=1024,

        # Controls diversity via nucleus sampling: 0.5 means half of all
        # likelihood-weighted options are considered.
        top_p=1,

        # A stop sequence is a predefined or user-specified text string that
        # signals an AI to stop generating content, ensuring its responses
        # remain focused and concise. Examples include punctuation marks and
        # markers like "[end]".
        stop=None,

        # If set, partial message deltas will be sent.
        stream=False,
    )

    # Print the completion returned by the LLM.
    print(chat_completion.choices[0].message.content)

asyncio.run(main())
```

### [Streaming an Async Chat Completion](#streaming-an-async-chat-completion)

You can combine the benefits of streaming and asynchronous processing by streaming completions asynchronously. This is particularly useful for applications that need to handle multiple concurrent conversations.

Python

```
import asyncio

from groq import AsyncGroq


async def main():
    client = AsyncGroq()

    stream = await client.chat.completions.create(
        #
        # Required parameters
        #
        messages=[
            # Set an optional system message. This sets the behavior of the
            # assistant and can be used to provide specific instructions for
            # how it should behave throughout the conversation.
            {
                "role": "system",
                "content": "You are a helpful assistant."
            },
            # Set a user message for the assistant to respond to.
            {
                "role": "user",
                "content": "Explain the importance of fast language models",
            }
        ],

        # The language model which will generate the completion.
        model="llama-3.3-70b-versatile",

        #
        # Optional parameters
        #

        # Controls randomness: lowering results in less random completions.
        # As the temperature approaches zero, the model will become
        # deterministic and repetitive.
        temperature=0.5,

        # The maximum number of tokens to generate. Requests can use up to
        # 2048 tokens shared between prompt and completion.
        max_completion_tokens=1024,

        # Controls diversity via nucleus sampling: 0.5 means half of all
        # likelihood-weighted options are considered.
        top_p=1,

        # A stop sequence is a predefined or user-specified text string that
        # signals an AI to stop generating content, ensuring its responses
        # remain focused and concise. Examples include punctuation marks and
        # markers like "[end]".
        stop=None,

        # If set, partial message deltas will be sent.
        stream=True,
    )

    # Print the incremental deltas returned by the LLM.
    async for chunk in stream:
        print(chunk.choices[0].delta.content, end="")

asyncio.run(main())
```

## [Structured Outputs and JSON](#structured-outputs-and-json)

Need reliable, type-safe JSON responses that match your exact schema? Groq's Structured Outputs feature is designed so that model responses strictly conform to your JSON Schema without validation or retry logic.

  
For complete guides on implementing structured outputs with JSON Schema or using JSON Object Mode, see our [structured outputs documentation](https://console.groq.com/docs/structured-outputs).

  
Key capabilities:

* **JSON Schema enforcement**: Responses match your schema exactly
* **Type-safe outputs**: No validation or retry logic needed
* **Programmatic refusal detection**: Handle safety-based refusals programmatically
* **JSON Object Mode**: Basic JSON output with prompt-guided structure
---
description: Learn why tools are essential for building powerful AI agents and how they enable LLMs to take action in the real world.
title: Tool Use Overview - GroqDocs
image: https://console.groq.com/og_cloudv5.jpg
---

# Tool Use

Applications using LLMs become much more powerful when the model can interact with external resources, such as APIs, databases, and the web, to gather dynamic data or to perform actions. **Tool use** (or function calling) is what transforms a language model from a conversational interface into an autonomous agent capable of taking action, accessing real-time information, and solving complex multi-step problems.

This doc starts with a high-level overview of tool use and then dives into the details of how tool use works. If you're already familiar with tool use, you can skip to the [How to Use Tools on the Groq API](#how-to-use-tools-on-the-groq-api) section.

## [How Tool Use Works](#how-tool-use-works)

There are a few important pieces in the tool calling process:

1. A request is made to the model with tool definitions
2. The model returns tool call requests
3. The tool is executed and results are returned to the model
4. The model evaluates the results and continues or completes

Let's break down each step in more detail.

### [1\. Initial Request with Tool Definitions](#1-initial-request-with-tool-definitions)

To use tools, the model must be provided with tool definitions. These tool definitions are in JSON schema format and are passed to the model via the `tools` parameter in the API request.

JSON

```
// Sample request body with tool definitions and messages
{
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "get_weather",
        "description": "Get current weather for a location",
        "parameters": {
          // JSON Schema object
          "type": "object",
          "properties": {
            "location": {
              "type": "string",
              "description": "City and state, e.g. San Francisco, CA"
            },
            "unit": {
              "type": "string",
              "enum": ["celsius", "fahrenheit"]
            }
          },
          "required": ["location"]
        }
      }
    }
  ],
  "messages": [
    {
      "role": "system",
      "content": "You are a weather assistant. Respond to the user question and use tools if needed to answer the query."
    },
    {
      "role": "user",
      "content": "What's the weather in San Francisco?" 
    }
  ],
}
```

**Key fields:**

* `name`: Function identifier
* `description`: Helps the model decide when to use this tool
* `parameters`: Function parameters defined as a JSON Schema object. Refer to [JSON Schema](https://json-schema.org/learn/getting-started-step-by-step#introduction-to-json-schema) for schema documentation.

### [2\. Model Returns Tool Call Requests](#2-model-returns-tool-call-requests)

When the model decides to use a tool, it returns structured tool calls in the response. The model returns a `tool_calls` array with the following fields:

JSON

```
{
  "role": "assistant",
  "tool_calls": [{
    "id": "call_abc123",
    "type": "function",
    "function": {
      "name": "get_weather",
      "arguments": "{\"location\": \"San Francisco, CA\", \"unit\": \"fahrenheit\"}"
    }
  }]
}
```

**Key fields:**

* `id`: Unique identifier you'll reference when returning results
* `function.name`: Which tool to execute
* `function.arguments`: JSON string of arguments (needs parsing)

### [3\. Tool Execution and Results](#3-tool-execution-and-results)

Application code will then execute the tool and create a new message with the results. This new message is appended to the conversation and sent back to the model.

JSON

```
{
  "role": "tool",
  # must match the `id` from the assistant's `tool_calls`
  "tool_call_id": "call_abc123",
  "name": "get_weather",
  "content": "{\"temperature\": 72, \"condition\": \"sunny\", \"unit\": \"fahrenheit\"}"
}
```

**Key connections:**

* The `tool` message's `tool_call_id` must match the `id` from the assistant's `tool_calls`
* `content` can be any string value. Different tools may return different types of data.
* The updated messages array is then sent back to the model for the next step

### [4\. Model Evaluates Results and Decides Next Steps](#4-model-evaluates-results-and-decides-next-steps)

The model is then provided with the updated messages array:

JSON

```
[
  {
    "role": "user",
    "content": "What's the weather in San Francisco?"
  },
  {
    "role": "assistant",
    "tool_calls": [{
      "id": "call_abc123",
      "type": "function",
      "function": {
        "name": "get_weather",
        "arguments": "{\"location\": \"San Francisco, CA\", \"unit\": \"fahrenheit\"}"
      }
    }]
  },
  {
    "role": "tool",
    "tool_call_id": "call_abc123",
    "name": "get_weather",
    "content": "{\"temperature\": 72, \"condition\": \"sunny\", \"unit\": \"fahrenheit\"}"
  }
]
```

The model then analyzes the tool results and either:

* Returns a final answer (no more `tool_calls`)
* Returns more tool call requests (loop continues)

JSON

```
{
  "role": "assistant",
  "content": "The weather in San Francisco is sunny and 72 degrees Fahrenheit."
}
```

This tool-calling sequence is normally implemented in your application code, but Groq suports a number of ways to call tools server-side which allow your application code to remain simple while still allowing you to use tools.

## [Supported Models](#supported-models)

All models hosted on Groq support tool use, and in general, we recommend the latest models for improved tool use capabilities:

| Model ID                                      | Local & Remote Tool Use Support? | Parallel Tool Use Support? | JSON Mode Support? | Built-In Tools Support? |
| --------------------------------------------- | -------------------------------- | -------------------------- | ------------------ | ----------------------- |
| moonshotai/kimi-k2-instruct-0905              | Yes                             | Yes                       | Yes               | No                     |
| openai/gpt-oss-20b                            | Yes                             | No                        | Yes               | Yes                    |
| openai/gpt-oss-120b                           | Yes                             | No                        | Yes               | Yes                    |
| openai/gpt-oss-safeguard-20b                  | Yes                             | No                        | Yes               | No                     |
| qwen/qwen3-32b                                | Yes                             | Yes                       | Yes               | No                     |
| meta-llama/llama-4-scout-17b-16e-instruct     | Yes                             | Yes                       | Yes               | No                     |
| meta-llama/llama-4-maverick-17b-128e-instruct | Yes                             | Yes                       | Yes               | No                     |
| llama-3.3-70b-versatile                       | Yes                             | Yes                       | Yes               | No                     |
| llama-3.1-8b-instant                          | Yes                             | Yes                       | Yes               | No                     |
| groq/compound                                 | No                              | N/A                        | Yes               | Yes                    |
| groq/compound-mini                            | No                              | N/A                        | Yes               | Yes                    |

## [How to Use Tools on the Groq API](#how-to-use-tools-on-the-groq-api)

Groq supports three distinct patterns for tool use, each suited for different use cases: Groq built-in tools, remote tool calling via MCP servers, and local tool calling.

### [1\. Groq Built-In Tools](#1-groq-builtin-tools)

Groq maintains a set of pre-built tools like web search, code execution, and browser automation that execute entirely on Groq's infrastructure. These tools require minimal configuration and no tool orchestration on your end. With one API call, you get a capable, real-time AI agent. All tool calls happen in a single API call  when provided configured to have access to built-in tools, the model autonomously calls built-in tools and handles the entire agentic loop internally.

**Ideal for:**

* Drop-in developer experience with zero setup
* Applications requiring the lowest possible latency
* Web search and browsing capabilities
* Safe code execution environments
* Single-call agentic responses

**Supported models:**

* `groq/compound` and `groq/compound-mini`
* `openai/gpt-oss-20b` and `openai/gpt-oss-120b`
  
[Groq Built-In Tools GuideFor more details, this guide covers how to use Groq's server-side tools for instant agentic capabilities](https://console.groq.com/docs/tool-use/built-in-tools) 

### [2\. Remote Tool Calling with MCP](#2-remote-tool-calling-with-mcp)

The [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) is an open standard that allows models to connect to and execute external tools. Each MCP server hosts a set of tools, providing endpoints to fetch their definitions and execute them without requiring the end user to implement the underlying tool logic.

Groq supports MCP tool discovery and execution server-side via remote tool calling. Similar to built-in tools, this allows you to use third-party tools with minimal configuration and no tool orchestration on your end. To use remote tools, you provide an MCP server configuration, which includes the MCP server URL and authentication headers. Groq's servers will connect to the MCP server, discover the available tools, pass them to the model, and execute any tools that are called server-side  all in a single API call.

**Ideal for:**

* Standardized integrations (GitHub, databases, external APIs)
* Tools maintained by third parties
* Sharing tools across multiple applications
* Accessing tools without hosting infrastructure
  
[Remote Tools and MCP GuideFor more details, this guide covers how to use MCP servers for third-party tool integrations](https://console.groq.com/docs/tool-use/remote-mcp) 

### [3\. Local Tool Calling (Function Calling)](#3-local-tool-calling-function-calling)

If you want the most control over tool execution logic, you can implement local tool calling. To do this, you manually write a set of functions and corresonding tool definitions. The tool definitions are provided to the model at inference time, and the model returns structured tool call requests (example provided above; a JSON object specifying which function to call and what arguments to use). Your application code then executes the function that corresponds to the tool call request locally and sends the results back to the model for the final response.

These functions can connect to external resources such as databases, APIs, and external services, but they are "local" in the sense that they are executed on the same machine as the application code. You can also connect to MCP servers locally to execute tools. This requires implementing code to discover tools from the MCP server, provide them to the model at inference time, routing any tool calls back to the MCP server for execution, and finally returning the results back to the model for the final response.

**Ideal for:**

* Custom business logic
* Internal APIs and databases
* Proprietary workflows
* Fine-grained control over security and execution
  
[Local Tool Calling GuideLearn how to implement custom tools that execute in your application code](https://console.groq.com/docs/tool-use/local-tool-calling) 

### [Comparison](#comparison)

| Pattern        | You Provide                       | Execution Location | Orchestration   | API Calls                   |
| -------------- | --------------------------------- | ------------------ | --------------- | --------------------------- |
| **Built-In**   | List of enabled built-in tools    | Groq servers       | Groq manages    | Single call                 |
| **Remote MCP** | MCP server URL + auth             | MCP server         | Groq manages    | Single call                 |
| **Local**      | Tool definitions + implementation | Your code          | You manage loop | Multiple (2+ per iteration) |

### [Parallel Tool Use](#parallel-tool-use)

Many models support **parallel tool use**, where multiple tools can be called simultaneously in a single request. This is crucial for efficient agentic systems:

**Without parallel tool use:**

tool-calling-loop

```
Query: "What's the weather in NYC and LA?"
Call 1: get_weather(location="NYC")       Wait for result
Call 2: get_weather(location="LA")        Wait for result
Final response
```

**With parallel tool use:**

tool-calling-loop

```
Query: "What's the weather in NYC and LA?"
Call 1: [get_weather(location="NYC"), get_weather(location="LA")]
Both execute simultaneously  Final response
```

Parallel tool use dramatically reduces latency for queries that require multiple tool calls.

### [Why Groq's Speed Matters](#why-groqs-speed-matters)

Because agentic workflows involve multiple inference calls, using Groq's fast inference can significantly improve the user experience of an agentic application:

* **Single tool call workflow**: 2 inference calls instead of 1 (first call to determine if a tool call is needed, second call to send the tool call results back to the model)
* **Multi-tool workflow**: 3-5+ inference calls
* **Complex agent loops**: 10+ inference calls

With traditional inference speeds of 10-30 tokens/second, multi-tool workflows can feel painfully slow. Groq's inference speed of **300-1,000+ tokens/second** makes these agentic experiences feel **instantaneous**.

## [What's Next?](#whats-next)

Now that you understand the fundamentals of tool use and agentic systems, explore the specific patterns for using tools on the Groq API:

[Groq Built-In ToolsUse web search, code execution, and more without setup](https://console.groq.com/docs/tool-use/built-in-tools)[Remote Tools and MCPConnect to MCP servers for standardized tool integrations](https://console.groq.com/docs/tool-use/remote-mcp)[Local Tool CallingDefine and execute custom tools in your application code](https://console.groq.com/docs/tool-use/local-tool-calling)[Compound SystemsPurpose-built agentic systems with built-in tools and orchestration](https://console.groq.com/docs/compound)